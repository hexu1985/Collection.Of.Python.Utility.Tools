{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caeea396-ab41-4293-8609-69da245b6bd9",
   "metadata": {},
   "source": [
    "# Python Lex Yacc手册"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ebc37-d6cc-4d0c-90f4-369a08d04bfb",
   "metadata": {},
   "source": [
    "### **1 前言和预备**\n",
    "\n",
    "本文指导你使用PLY进行词法分析和语法解析的，鉴于解析本身是个复杂性的事情，在你使用PLY投入大规模的开发前，我强烈建议你完整地阅读或者浏览本文档。\n",
    "\n",
    "PLY-3.0能同时兼容Python2和Python3。需要注意的是，对于Python3的支持是新加入的，还没有广泛的测试（尽管所有的例子和单元测试都能够在Pythone3下通过）。如果你使用的是Python2，应该使用Python2.4以上版本，虽然，PLY最低能够支持到Python2.2，不过一些可选的功能需要新版本模块的支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2535fd9-f659-4844-bf55-258229cbf610",
   "metadata": {},
   "source": [
    "### **2 介绍**\n",
    "\n",
    "PLY是纯粹由Python实现的Lex和yacc（流行的编译器构建工具）。PLY的设计目标是尽可能的沿袭传统lex和yacc工具的工作方式，包括支持LALR(1)分析法、提供丰富的输入验证、错误报告和诊断。因此，如果你曾经在其他编程语言下使用过yacc，你应该能够很容易的迁移到PLY上。\n",
    "\n",
    "2001年，我在芝加哥大学教授“编译器简介”课程时开发了的早期的PLY。学生们使用Python和PLY构建了一个类似Pascal的语言的完整编译器，其中的语言特性包括：词法分析、语法分析、类型检查、类型推断、嵌套作用域，并针对SPARC处理器生成目标代码等。最终他们大约实现了30种不同的编译器！PLY在接口设计上影响使用的问题也被学生们所提出。从2001年以来，PLY继续从用户的反馈中不断改进。为了适应对未来的改进需求，PLY3.0在原来基础上进行了重大的重构。\n",
    "\n",
    "由于PLY是作为教学工具来开发的，你会发现它对于标记和语法规则是相当严谨的，这一定程度上是为了帮助新手用户找出常见的编程错误。不过，高级用户也会发现这有助于处理真实编程语言的复杂语法。还需要注意的是，PLY没有提供太多花哨的东西（例如，自动构建抽象语法树和遍历树），我也不认为它是个分析框架。相反，你会发现它是一个用Python实现的，基本的，但能够完全胜任的lex/yacc。\n",
    "\n",
    "本文的假设你多少熟悉分析理论、语法制导的翻译、基于其他编程语言使用过类似lex和yacc的编译器构建工具。如果你对这些东西不熟悉，你可能需要先去一些书籍中学习一些基础，比如：Aho, Sethi和Ullman的《Compilers: Principles, Techniques, and Tools》（《编译原理》），和O'Reilly'出版的John Levine的《lex and yacc》。事实上，《lex and yacc》和PLY使用的概念几乎相同。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788eea6-8dcb-4607-b710-519fefb2b5aa",
   "metadata": {},
   "source": [
    "### **3 PLY概要**\n",
    "\n",
    "PLY包含两个独立的模块：lex.py和yacc.py，都定义在ply包下。lex.py模块用来将输入字符通过一系列的正则表达式分解成标记序列，yacc.py通过一些上下文无关的文法来识别编程语言语法。yacc.py使用LR解析法，并使用LALR(1)算法（默认）或者SLR算法生成分析表。\n",
    "\n",
    "这两个工具是为了一起工作的。lex.py通过向外部提供`token()`方法作为接口，方法每次会从输入中返回下一个有效的标记。yacc.py将会不断的调用这个方法来获取标记并匹配语法规则。yacc.py的的功能通常是生成抽象语法树(AST)，不过，这完全取决于用户，如果需要，yacc.py可以直接用来完成简单的翻译。\n",
    "\n",
    "就像相应的unix工具，yacc.py提供了大多数你期望的特性，其中包括：丰富的错误检查、语法验证、支持空产生式、错误的标记、通过优先级规则解决二义性。事实上，传统yacc能够做到的PLY都应该支持。\n",
    "\n",
    "yacc.py与Unix下的yacc的主要不同之处在于，yacc.py没有包含一个独立的代码生成器，而是在PLY中依赖反射来构建词法分析器和语法解析器。不像传统的lex/yacc工具需要一个独立的输入文件，并将之转化成一个源文件，Python程序必须是一个可直接可用的程序，这意味着不能有额外的源文件和特殊的创建步骤（像是那种执行yacc命令来生成Python代码）。又由于生成分析表开销较大，PLY会缓存生成的分析表，并将它们保存在独立的文件中，除非源文件有变化，会重新生成分析表，否则将从缓存中直接读取。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0cddf-c8ad-476c-91c7-a8671b57e5d4",
   "metadata": {},
   "source": [
    "### **4 Lex**\n",
    "\n",
    "lex.py是用来将输入字符串标记化。例如，假设你正在设计一个编程语言，用户的输入字符串如下：\n",
    "\n",
    "```ini\n",
    "x = 3 + 42 * (s - t)\n",
    "```\n",
    "\n",
    "标记器将字符串分割成独立的标记：\n",
    "\n",
    "```bash\n",
    "'x','=', '3', '+', '42', '*', '(', 's', '-', 't', ')'\n",
    "```\n",
    "\n",
    "标记通常用一组名字来命名和表示：\n",
    "\n",
    "```bash\n",
    "'ID','EQUALS','NUMBER','PLUS','NUMBER','TIMES','LPAREN','ID','MINUS','ID','RPAREN'\n",
    "```\n",
    "\n",
    "将标记名和标记值本身组合起来：\n",
    "\n",
    "```rust\n",
    "('ID','x'), ('EQUALS','='), ('NUMBER','3'),('PLUS','+'), ('NUMBER','42), ('TIMES','*'),('LPAREN','('), ('ID','s'),('MINUS','-'),('ID','t'), ('RPAREN',')\n",
    "```\n",
    "\n",
    "正则表达式是描述标记规则的典型方法，下一节展示如何用lex.py实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ac06b-33ea-4133-bc49-a60529556c2c",
   "metadata": {},
   "source": [
    "#### **4.1 Lex的例子**\n",
    "\n",
    "下面的例子展示了如何使用lex.py对输入进行标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3628d196-fd8a-4096-9dc9-dfaf8ecd4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# calclex.py\n",
    "#\n",
    "# tokenizer for a simple expression evaluator for\n",
    "# numbers and +,-,*,/\n",
    "# ------------------------------------------------------------\n",
    "import ply.lex as lex\n",
    "\n",
    "# List of token names.   This is always required\n",
    "tokens = (\n",
    "   'NUMBER',\n",
    "   'PLUS',\n",
    "   'MINUS',\n",
    "   'TIMES',\n",
    "   'DIVIDE',\n",
    "   'LPAREN',\n",
    "   'RPAREN',\n",
    ")\n",
    "\n",
    "# Regular expression rules for simple tokens\n",
    "t_PLUS    = r'\\+'\n",
    "t_MINUS   = r'-'\n",
    "t_TIMES   = r'\\*'\n",
    "t_DIVIDE  = r'/'\n",
    "t_LPAREN  = r'\\('\n",
    "t_RPAREN  = r'\\)'\n",
    "\n",
    "# A regular expression rule with some action code\n",
    "def t_NUMBER(t):\n",
    "    r'\\d+'\n",
    "    t.value = int(t.value)    \n",
    "    return t\n",
    "\n",
    "# Define a rule so we can track line numbers\n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)\n",
    "\n",
    "# A string containing ignored characters (spaces and tabs)\n",
    "t_ignore  = ' \\t'\n",
    "\n",
    "# Error handling rule\n",
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "\n",
    "# Build the lexer\n",
    "# lexer = lex.lex()  # will cause error in jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f85b8-0856-4b91-bbe9-0ca697408268",
   "metadata": {},
   "source": [
    "为了使lexer工作，你需要给定一个输入，并传递给input()方法。然后，重复调用token()方法来获取标记序列，下面的代码展示了这种用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f833737c-f509-4d4b-aa2a-aa6a1dce9964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,3,2,1)\n",
      "LexToken(PLUS,'+',2,3)\n",
      "LexToken(NUMBER,4,2,5)\n",
      "LexToken(TIMES,'*',2,7)\n",
      "LexToken(NUMBER,10,2,9)\n",
      "LexToken(PLUS,'+',3,14)\n",
      "LexToken(MINUS,'-',3,16)\n",
      "LexToken(NUMBER,20,3,17)\n",
      "LexToken(TIMES,'*',3,20)\n",
      "LexToken(NUMBER,2,3,21)\n"
     ]
    }
   ],
   "source": [
    "from calclex import lexer\n",
    "\n",
    "# Test it out\n",
    "data = '''\n",
    "3 + 4 * 10\n",
    "  + -20 *2\n",
    "'''\n",
    "\n",
    "# Give the lexer some input\n",
    "lexer.input(data)\n",
    "\n",
    "# Tokenize\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    if not tok: break      # No more input\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793beb1a-ffe6-4f87-93a3-965a30d22c31",
   "metadata": {},
   "source": [
    "Lexers也同时支持迭代，你可以把上面的循环写成这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b44a20-00a3-4a2d-a8b3-a907bcb3d3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,3,5,1)\n",
      "LexToken(PLUS,'+',5,3)\n",
      "LexToken(NUMBER,4,5,5)\n",
      "LexToken(TIMES,'*',5,7)\n",
      "LexToken(NUMBER,10,5,9)\n",
      "LexToken(PLUS,'+',6,14)\n",
      "LexToken(MINUS,'-',6,16)\n",
      "LexToken(NUMBER,20,6,17)\n",
      "LexToken(TIMES,'*',6,20)\n",
      "LexToken(NUMBER,2,6,21)\n"
     ]
    }
   ],
   "source": [
    "from calclex import lexer\n",
    "\n",
    "# Test it out\n",
    "data = '''\n",
    "3 + 4 * 10\n",
    "  + -20 *2\n",
    "'''\n",
    "\n",
    "# Give the lexer some input\n",
    "lexer.input(data)\n",
    "\n",
    "for tok in lexer:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9fa83-0e08-4311-8001-689cdba5da91",
   "metadata": {},
   "source": [
    "由`lexer.token()`方法返回的标记是`LexToken`类型的实例，拥有`tok.type`,`tok.value`,`tok.lineno`和`tok.lexpos`属性，下面的代码展示了如何访问这些属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c1c3fa-e60b-43a4-9283-538a116e6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER 3 8 1\n",
      "PLUS + 8 3\n",
      "NUMBER 4 8 5\n",
      "TIMES * 8 7\n",
      "NUMBER 10 8 9\n",
      "PLUS + 9 14\n",
      "MINUS - 9 16\n",
      "NUMBER 20 9 17\n",
      "TIMES * 9 20\n",
      "NUMBER 2 9 21\n"
     ]
    }
   ],
   "source": [
    "from calclex import lexer\n",
    "\n",
    "# Test it out\n",
    "data = '''\n",
    "3 + 4 * 10\n",
    "  + -20 *2\n",
    "'''\n",
    "\n",
    "# Give the lexer some input\n",
    "lexer.input(data)\n",
    "\n",
    "# Tokenize\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    if not tok: break      # No more input\n",
    "    print(tok.type, tok.value, tok.lineno, tok.lexpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7887e-d893-40dd-9f4a-09177e0dfa2b",
   "metadata": {},
   "source": [
    "`tok.type`和`tok.value`属性表示标记本身的类型和值。`tok.lineno`和`tok.lexpos`属性包含了标记的位置信息，`tok.lexpos`表示标记相对于输入串起始位置的偏移。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad202c7-5e46-47ad-880a-b5fa1f9cee1d",
   "metadata": {},
   "source": [
    "#### **4.2 标记列表**\n",
    "\n",
    "词法分析器必须提供一个标记的列表，这个列表将所有可能的标记告诉分析器，用来执行各种验证，同时也提供给yacc.py作为终结符。\n",
    "\n",
    "在上面的例子中，是这样给定标记列表的：\n",
    "\n",
    "```lisp\n",
    "tokens = (\n",
    "   'NUMBER',\n",
    "   'PLUS',\n",
    "   'MINUS',\n",
    "   'TIMES',\n",
    "   'DIVIDE',\n",
    "   'LPAREN',\n",
    "   'RPAREN',\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7ac17-dae4-44f3-bb49-ef9ebd8638ff",
   "metadata": {},
   "source": [
    "#### **4.3 标记的规则**\n",
    "\n",
    "每种标记用一个正则表达式规则来表示，每个规则需要以\"t_\"开头声明，表示该声明是对标记的规则定义。对于简单的标记，可以定义成这样（在Python中使用raw string能比较方便的书写正则表达式）：\n",
    "\n",
    "```python\n",
    "t_PLUS = r'\\+'\n",
    "```\n",
    "\n",
    "这里，紧跟在t_后面的单词，必须跟标记列表中的某个标记名称对应。如果需要执行动作的话，规则可以写成一个方法。例如，下面的规则匹配数字字串，并且将匹配的字符串转化成Python的整型：\n",
    "\n",
    "```python\n",
    "def t_NUMBER(t):\n",
    "    r'\\d+'\n",
    "    t.value = int(t.value)\n",
    "    return t\n",
    "```\n",
    "\n",
    "如果使用方法的话，正则表达式写成方法的文档字符串。方法总是需要接受一个`LexToken`实例的参数，该实例有一个`t.type`的属性（字符串表示）来表示标记的类型名称，`t.value`是标记值（匹配的实际的字符串），`t.lineno`表示当前在源输入串中的作业行，`t.lexpos`表示标记相对于输入串起始位置的偏移。默认情况下，`t.type`是以t_开头的变量或方法的后面部分。方法可以在方法体里面修改这些属性。但是，如果这样做，应该返回结果token，否则，标记将被丢弃。\n",
    "在lex内部，lex.py用`re`模块处理模式匹配，在构造最终的完整的正则式的时候，用户提供的规则按照下面的顺序加入：\n",
    "\n",
    "1. 所有由方法定义的标记规则，按照他们的出现顺序依次加入\n",
    "2. 由字符串变量定义的标记规则按照其正则式长度倒序后，依次加入（长的先入）\n",
    "\n",
    "顺序的约定对于精确匹配是必要的。比如，如果你想区分‘=’和‘==’，你需要确保‘==’优先检查。如果用字符串来定义这样的表达式的话，通过将较长的正则式先加入，可以帮助解决这个问题。用方法定义标记，可以显示地控制哪个规则优先检查。\n",
    "为了处理保留字，你应该写一个单一的规则来匹配这些标识，并在方法里面作特殊的查询：\n",
    "\n",
    "```python\n",
    "reserved = {\n",
    "   'if' : 'IF',\n",
    "   'then' : 'THEN',\n",
    "   'else' : 'ELSE',\n",
    "   'while' : 'WHILE',\n",
    "   ...\n",
    "}\n",
    "\n",
    "tokens = ['LPAREN','RPAREN',...,'ID'] + list(reserved.values())\n",
    "\n",
    "def t_ID(t):\n",
    "    r'[a-zA-Z_][a-zA-Z_0-9]*'\n",
    "    t.type = reserved.get(t.value,'ID')    # Check for reserved words\n",
    "    return t\n",
    "```\n",
    "\n",
    "这样做可以大大减少正则式的个数，并稍稍加快处理速度。注意：你应该避免为保留字编写单独的规则，例如，如果你像下面这样写：\n",
    "\n",
    "```python\n",
    "t_FOR   = r'for'\n",
    "t_PRINT = r'print'\n",
    "```\n",
    "\n",
    "但是，这些规则照样也能够匹配以这些字符开头的单词，比如'forget'或者'printed'，这通常不是你想要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc134f-b6c6-4b94-9a77-ea1d3c74360f",
   "metadata": {},
   "source": [
    "#### **4.4 标记的值**\n",
    "\n",
    "标记被lex返回后，它们的值被保存在`value`属性中。正常情况下，`value`是匹配的实际文本。事实上，`value`可以被赋为任何Python支持的类型。例如，当扫描到标识符的时候，你可能不仅需要返回标识符的名字，还需要返回其在符号表中的位置，可以像下面这样写：\n",
    "\n",
    "```python\n",
    "def t_ID(t):\n",
    "    ...\n",
    "    # Look up symbol table information and return a tuple\n",
    "    t.value = (t.value, symbol_lookup(t.value))\n",
    "    ...\n",
    "    return t\n",
    "```\n",
    "\n",
    "需要注意的是，不推荐用其他属性来保存值，因为yacc.py模块只会暴露出标记的`value`属性，访问其他属性会变得不自然。如果想保存多种属性，可以将元组、字典、或者对象实例赋给value。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13556b-8cc1-4734-907c-68999a23fc9e",
   "metadata": {},
   "source": [
    "#### **4.5 丢弃标记**\n",
    "\n",
    "想丢弃像注释之类的标记，只要不返回value就行了，像这样：\n",
    "\n",
    "```python\n",
    "def t_COMMENT(t):\n",
    "    r'\\#.*'\n",
    "    pass\n",
    "    # No return value. Token discarded\n",
    "```\n",
    "\n",
    "为标记声明添加\"ignore_\"前缀同样可以达到目的：\n",
    "\n",
    "```python\n",
    "t_ignore_COMMENT = r'\\#.*'\n",
    "```\n",
    "\n",
    "如果有多种文本需要丢弃，建议使用方法来定义规则，因为方法能够提供更精确的匹配优先级控制（方法根据出现的顺序，而字符串的正则表达式依据正则表达式的长度）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b110d6-07e6-4444-86af-6e4c8db4ff83",
   "metadata": {},
   "source": [
    "#### **4.6 行号和位置信息**\n",
    "\n",
    "默认情况下，lex.py对行号一无所知。因为lex.py根本不知道何为\"行\"的概念（换行符本身也作为文本的一部分）。不过，可以通过写一个特殊的规则来记录行号：\n",
    "\n",
    "```python\n",
    "# Define a rule so we can track line numbers\n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)\n",
    "```\n",
    "\n",
    "在这个规则中，当前lexer对象t.lexer的lineno属性被修改了，而且空行被简单的丢弃了，因为没有任何的返回。\n",
    "\n",
    "lex.py也不自动做列跟踪。但是，位置信息被记录在了每个标记对象的lexpos属性中，这样，就有可能来计算列信息了。例如：每当遇到新行的时候就重置列值：\n",
    "\n",
    "```python\n",
    "# Compute column. \n",
    "#     input is the input text string\n",
    "#     token is a token instance\n",
    "def find_column(input,token):\n",
    "    last_cr = input.rfind('\\n',0,token.lexpos)\n",
    "    if last_cr < 0:\n",
    "        last_cr = 0\n",
    "    column = (token.lexpos - last_cr) + 1\n",
    "    return column\n",
    "```\n",
    "\n",
    "通常，计算列的信息是为了指示上下文的错误位置，所以只在必要时有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f0a5b-a75e-4f56-9943-50ca1d077656",
   "metadata": {},
   "source": [
    "#### **4.7 忽略字符**\n",
    "\n",
    "`t_ignore`规则比较特殊，是lex.py所保留用来忽略字符的，通常用来跳过空白或者不需要的字符。虽然可以通过定义像`t_newline()`这样的规则来完成相同的事情，不过使用t_ignore能够提供较好的词法分析性能，因为相比普通的正则式，它被特殊化处理了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c01e0-aacf-423d-a1d0-70579b7270a2",
   "metadata": {},
   "source": [
    "#### **4.8 字面字符**\n",
    "\n",
    "字面字符可以通过在词法模块中定义一个`literals`变量做到，例如：\n",
    "\n",
    "```ini\n",
    "literals = [ '+','-','*','/' ]\n",
    "```\n",
    "\n",
    "或者\n",
    "\n",
    "```ini\n",
    "literals = \"+-*/\"\n",
    "```\n",
    "\n",
    "字面字符是指单个字符，表示把字符本身作为标记，标记的`type`和`value`都是字符本身。不过，字面字符是在其他正则式之后被检查的，因此如果有规则是以这些字符开头的，那么这些规则的优先级较高。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c191e-5861-4327-8b94-a592135c9146",
   "metadata": {},
   "source": [
    "#### **4.9 错误处理**\n",
    "\n",
    "最后，在词法分析中遇到非法字符时，`t_error()`用来处理这类错误。这种情况下，`t.value`包含了余下还未被处理的输入字串，在之前的例子中，错误处理方法是这样的：\n",
    "\n",
    "```python\n",
    "# Error handling rule\n",
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "```\n",
    "\n",
    "这个例子中，我们只是简单的输出不合法的字符，并且通过调用`t.lexer.skip(1)`跳过一个字符。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159a231-81c6-46a8-9656-df3f6e012761",
   "metadata": {},
   "source": [
    "#### **4.10 构建和使用lexer**\n",
    "\n",
    "函数`lex.lex()`使用Python的反射机制读取调用上下文中的正则表达式，来创建lexer。lexer一旦创建好，有两个方法可以用来控制lexer对象：\n",
    "\n",
    "- `lexer.input(data)` 重置lexer和输入字串\n",
    "- `lexer.token()` 返回下一个`LexToken`类型的标记实例，如果进行到输入字串的尾部时将返回`None`\n",
    "\n",
    "推荐直接在`lex()`函数返回的lexer对象上调用上述接口，尽管也可以向下面这样用模块级别的`lex.input()`和`lex.token()`：\n",
    "\n",
    "```lua\n",
    "lex.lex()\n",
    "lex.input(sometext)\n",
    "while 1:\n",
    "    tok = lex.token()\n",
    "    if not tok: break\n",
    "    print(tok)\n",
    "```\n",
    "\n",
    "在这个例子中，`lex.input()`和`lex.token()`是模块级别的方法，在lex模块中，`input()`和`token()`方法绑定到最新创建的lexer对象的对应方法上。最好不要这样用，因为这种接口可能不知道在什么时候就失效"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ef483-561b-4703-9bc9-e8d29bfe5935",
   "metadata": {},
   "source": [
    "#### **4.11 @TOKEN装饰器**\n",
    "\n",
    "在一些应用中，你可能需要定义一系列辅助的记号来构建复杂的正则表达式，例如：\n",
    "\n",
    "```python\n",
    "digit            = r'([0-9])'\n",
    "nondigit         = r'([_A-Za-z])'\n",
    "identifier       = r'(' + nondigit + r'(' + digit + r'|' + nondigit + r')*)'        \n",
    "\n",
    "def t_ID(t):\n",
    "    # want docstring to be identifier above. ?????\n",
    "    ...\n",
    "```\n",
    "\n",
    "在这个例子中，我们希望ID的规则引用上面的已有的变量。然而，使用文档字符串无法做到，为了解决这个问题，你可以使用**@TOKEN**装饰器：\n",
    "\n",
    "```python\n",
    "from ply.lex import TOKEN\n",
    "\n",
    "@TOKEN(identifier)\n",
    "def t_ID(t):\n",
    "    ...\n",
    "```\n",
    "\n",
    "装饰器可以将identifier关联到t_ID()的文档字符串上以使lex.py正常工作，一种等价的做法是直接给文档字符串赋值：\n",
    "\n",
    "```cpp\n",
    "def t_ID(t):\n",
    "    ...\n",
    "\n",
    "t_ID.__doc__ = identifier\n",
    "```\n",
    "\n",
    "注意：@TOKEN装饰器需要Python-2.4以上的版本。如果你在意老版本Python的兼容性问题，使用上面的等价办法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3ad2d-7bbe-4801-bfb2-8c6adeb1e032",
   "metadata": {},
   "source": [
    "#### **4.12 优化模式**\n",
    "\n",
    "为了提高性能，你可能希望使用Python的优化模式（比如，使用-o选项执行Python）。然而，这样的话，Python会忽略文档字串，这是lex.py的特殊问题，可以通过在创建lexer的时候使用`optimize`选项：\n",
    "\n",
    "```ini\n",
    "lexer = lex.lex(optimize=1)\n",
    "```\n",
    "\n",
    "接着，用Python常规的模式运行，这样，lex.py会在当前目录下创建一个lextab.py文件，这个文件会包含所有的正则表达式规则和词法分析阶段的分析表。然后，lextab.py可以被导入用来构建lexer。这种方法大大改善了词法分析程序的启动时间，而且可以在Python的优化模式下工作。\n",
    "\n",
    "想要更改生成的文件名，使用如下参数：\n",
    "\n",
    "```ini\n",
    "lexer = lex.lex(optimize=1,lextab=\"footab\")\n",
    "```\n",
    "\n",
    "在优化模式下执行，需要注意的是lex会被禁用大多数的错误检查。因此，建议只在确保万事俱备准备发布最终代码时使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992771d-5267-48e3-8f0c-fdb8e98cfa6d",
   "metadata": {},
   "source": [
    "#### **4.13 调试**\n",
    "\n",
    "如果想要调试，可以使lex()运行在调试模式：\n",
    "\n",
    "```ini\n",
    "lexer = lex.lex(debug=1)\n",
    "```\n",
    "\n",
    "这将打出一些调试信息，包括添加的规则、最终的正则表达式和词法分析过程中得到的标记。\n",
    "\n",
    "除此之外，lex.py有一个简单的主函数，不但支持对命令行参数输入的字串进行扫描，还支持命令行参数指定的文件名：\n",
    "\n",
    "```java\n",
    "if __name__ == '__main__':\n",
    "     lex.runmain()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f2cd3-666e-4c1a-ab24-a24fbd51c15f",
   "metadata": {},
   "source": [
    "#### **4.14 其他方式定义词法规则**\n",
    "\n",
    "上面的例子，词法分析器都是在单个的Python模块中指定的。如果你想将标记的规则放到不同的模块，使用`module`关键字参数。例如，你可能有一个专有的模块，包含了标记的规则：\n",
    "\n",
    "```python\n",
    "# module: tokrules.py\n",
    "# This module just contains the lexing rules\n",
    "\n",
    "# List of token names.   This is always required\n",
    "tokens = (\n",
    "   'NUMBER',\n",
    "   'PLUS',\n",
    "   'MINUS',\n",
    "   'TIMES',\n",
    "   'DIVIDE',\n",
    "   'LPAREN',\n",
    "   'RPAREN',\n",
    ")\n",
    "\n",
    "# Regular expression rules for simple tokens\n",
    "t_PLUS    = r'\\+'\n",
    "t_MINUS   = r'-'\n",
    "t_TIMES   = r'\\*'\n",
    "t_DIVIDE  = r'/'\n",
    "t_LPAREN  = r'\\('\n",
    "t_RPAREN  = r'\\)'\n",
    "\n",
    "# A regular expression rule with some action code\n",
    "def t_NUMBER(t):\n",
    "    r'\\d+'\n",
    "    t.value = int(t.value)    \n",
    "    return t\n",
    "\n",
    "# Define a rule so we can track line numbers\n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)\n",
    "\n",
    "# A string containing ignored characters (spaces and tabs)\n",
    "t_ignore  = ' \\t'\n",
    "\n",
    "# Error handling rule\n",
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd524b-7846-42b2-80ca-605d0e3f8c88",
   "metadata": {},
   "source": [
    "现在，如果你想要从不同的模块中构建分析器，应该这样（在交互模式下）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3422fe-0f7a-41f2-b85d-decd8f79bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,3,1,0)\n",
      "LexToken(PLUS,'+',1,2)\n",
      "LexToken(NUMBER,4,1,4)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    ">>> import tokrules\n",
    ">>> lexer = lex.lex(module=tokrules)\n",
    ">>> lexer.input(\"3 + 4\")\n",
    ">>> print(lexer.token())\n",
    ">>> print(lexer.token())\n",
    ">>> print(lexer.token())\n",
    ">>> print(lexer.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fba0bd-66c8-4c66-b7b3-2b1c9f77fd03",
   "metadata": {},
   "source": [
    "`module`选项也可以指定类型的实例，例如：\n",
    "\n",
    "```python\n",
    "# mylexer.py\n",
    "import ply.lex as lex\n",
    "\n",
    "class MyLexer:\n",
    "    # List of token names.   This is always required\n",
    "    tokens = (\n",
    "       'NUMBER',\n",
    "       'PLUS',\n",
    "       'MINUS',\n",
    "       'TIMES',\n",
    "       'DIVIDE',\n",
    "       'LPAREN',\n",
    "       'RPAREN',\n",
    "    )\n",
    "\n",
    "    # Regular expression rules for simple tokens\n",
    "    t_PLUS    = r'\\+'\n",
    "    t_MINUS   = r'-'\n",
    "    t_TIMES   = r'\\*'\n",
    "    t_DIVIDE  = r'/'\n",
    "    t_LPAREN  = r'\\('\n",
    "    t_RPAREN  = r'\\)'\n",
    "\n",
    "    # A regular expression rule with some action code\n",
    "    # Note addition of self parameter since we're in a class\n",
    "    def t_NUMBER(self,t):\n",
    "        r'\\d+'\n",
    "        t.value = int(t.value)    \n",
    "        return t\n",
    "\n",
    "    # Define a rule so we can track line numbers\n",
    "    def t_newline(self,t):\n",
    "        r'\\n+'\n",
    "        t.lexer.lineno += len(t.value)\n",
    "\n",
    "    # A string containing ignored characters (spaces and tabs)\n",
    "    t_ignore  = ' \\t'\n",
    "\n",
    "    # Error handling rule\n",
    "    def t_error(self,t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        t.lexer.skip(1)\n",
    "\n",
    "    # Build the lexer\n",
    "    def build(self,**kwargs):\n",
    "        self.lexer = lex.lex(module=self, **kwargs)\n",
    "    \n",
    "    # Test it output\n",
    "    def test(self,data):\n",
    "        self.lexer.input(data)\n",
    "        while True:\n",
    "             tok = self.lexer.token()\n",
    "             if not tok: break\n",
    "             print(tok)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f2f71a-46f2-444c-b25c-65cfa1eb994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,3,1,0)\n",
      "LexToken(PLUS,'+',1,2)\n",
      "LexToken(NUMBER,4,1,4)\n"
     ]
    }
   ],
   "source": [
    "# Build the lexer and try it out\n",
    "from mylexer import MyLexer\n",
    "\n",
    "m = MyLexer()\n",
    "m.build()           # Build the lexer\n",
    "m.test(\"3 + 4\")     # Test it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c758ae-f5ec-457f-9269-44473958f012",
   "metadata": {},
   "source": [
    "当从类中定义lexer，你需要创建类的实例，而不是类本身。这是因为，lexer的方法只有被绑定（bound-methods）对象后才能使PLY正常工作。\n",
    "\n",
    "当给`lex()`方法使用`module`选项时，PLY使用`dir()`方法，从对象中获取符号信息，因为不能直接访问对象的`__dict__`属性。（译者注：可能是因为兼容性原因`，__dict__`这个方法可能不存在）\n",
    "\n",
    "最后，如果你希望保持较好的封装性，但不希望什么东西都写在类里面，lexers可以在闭包中定义，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec1e75a-5d17-412a-a802-d8ddc5436772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.lex as lex\n",
    "\n",
    "# List of token names.   This is always required\n",
    "tokens = (\n",
    "  'NUMBER',\n",
    "  'PLUS',\n",
    "  'MINUS',\n",
    "  'TIMES',\n",
    "  'DIVIDE',\n",
    "  'LPAREN',\n",
    "  'RPAREN',\n",
    ")\n",
    "\n",
    "def MyLexer():\n",
    "    # Regular expression rules for simple tokens\n",
    "    t_PLUS    = r'\\+'\n",
    "    t_MINUS   = r'-'\n",
    "    t_TIMES   = r'\\*'\n",
    "    t_DIVIDE  = r'/'\n",
    "    t_LPAREN  = r'\\('\n",
    "    t_RPAREN  = r'\\)'\n",
    "\n",
    "    # A regular expression rule with some action code\n",
    "    def t_NUMBER(t):\n",
    "        r'\\d+'\n",
    "        t.value = int(t.value)    \n",
    "        return t\n",
    "\n",
    "    # Define a rule so we can track line numbers\n",
    "    def t_newline(t):\n",
    "        r'\\n+'\n",
    "        t.lexer.lineno += len(t.value)\n",
    "\n",
    "    # A string containing ignored characters (spaces and tabs)\n",
    "    t_ignore  = ' \\t'\n",
    "\n",
    "    # Error handling rule\n",
    "    def t_error(t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        t.lexer.skip(1)\n",
    "\n",
    "    # Build the lexer from my environment and return it    \n",
    "    return lex.lex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7faa0c1-a124-42ef-88b3-7c9ce6dd1cc0",
   "metadata": {},
   "source": [
    "#### **4.15 额外状态维护**\n",
    "\n",
    "在你的词法分析器中，你可能想要维护一些状态。这可能包括模式设置，符号表和其他细节。例如，假设你想要跟踪`NUMBER`标记的出现个数。\n",
    "\n",
    "一种方法是维护一个全局变量：\n",
    "\n",
    "```python\n",
    "num_count = 0\n",
    "def t_NUMBER(t):\n",
    "    r'\\d+'\n",
    "    global num_count\n",
    "    num_count += 1\n",
    "    t.value = int(t.value)    \n",
    "    return t\n",
    "```\n",
    "\n",
    "如果你不喜欢全局变量，另一个记录信息的地方是lexer对象内部。可以通过当前标记的lexer属性访问：\n",
    "\n",
    "```python\n",
    "def t_NUMBER(t):\n",
    "    r'\\d+'\n",
    "    t.lexer.num_count += 1     # Note use of lexer attribute\n",
    "    t.value = int(t.value)    \n",
    "    return t\n",
    "\n",
    "lexer = lex.lex()\n",
    "lexer.num_count = 0            # Set the initial count\n",
    "```\n",
    "\n",
    "上面这样做的优点是当同时存在多个lexer实例的情况下，简单易行。不过这看上去似乎是严重违反了面向对象的封装原则。lexer的内部属性（除了`lineno`）都是以lex开头命名的（`lexdata`、`lexpos`）。因此，只要不以lex开头来命名属性就很安全的。\n",
    "\n",
    "如果你不喜欢给lexer对象赋值，你可以自定义你的lexer类型，就像前面看到的那样：\n",
    "\n",
    "```python\n",
    "class MyLexer:\n",
    "    ...\n",
    "    def t_NUMBER(self,t):\n",
    "        r'\\d+'\n",
    "        self.num_count += 1\n",
    "        t.value = int(t.value)    \n",
    "        return t\n",
    "\n",
    "    def build(self, **kwargs):\n",
    "        self.lexer = lex.lex(object=self,**kwargs)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_count = 0\n",
    "```\n",
    "\n",
    "如果你的应用会创建很多lexer的实例，并且需要维护很多状态，上面的类可能是最容易管理的。\n",
    "\n",
    "状态也可以用闭包来管理，比如，在Python3中：\n",
    "\n",
    "```python\n",
    "def MyLexer():\n",
    "    num_count = 0\n",
    "    ...\n",
    "    def t_NUMBER(t):\n",
    "        r'\\d+'\n",
    "        nonlocal num_count\n",
    "        num_count += 1\n",
    "        t.value = int(t.value)    \n",
    "        return t\n",
    "    ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc4e4b-9706-4506-b819-74134732cab4",
   "metadata": {},
   "source": [
    "#### **4.16 Lexer克隆**\n",
    "\n",
    "如果有必要的话，lexer对象可以通过`clone()`方法来复制：\n",
    "\n",
    "```makefile\n",
    "lexer = lex.lex()\n",
    "...\n",
    "newlexer = lexer.clone()\n",
    "```\n",
    "\n",
    "当lexer被克隆后，复制品能够精确的保留输入串和内部状态，不过，新的lexer可以接受一个不同的输出字串，并独立运作起来。这在几种情况下也许有用：当你在编写的解析器或编译器涉及到递归或者回退处理时，你需要扫描先前的部分，你可以clone并使用复制品，或者你在实现某种预编译处理，可以clone一些lexer来处理不同的输入文件。\n",
    "\n",
    "创建克隆跟重新调用`lex.lex()`的不同点在于，PLY不会重新构建任何的内部分析表或者正则式。当lexer是用类或者闭包创建的，需要注意类或闭包本身的的状态。换句话说你要注意新创建的lexer会共享原始lexer的这些状态，比如：\n",
    "\n",
    "```ini\n",
    "m = MyLexer()\n",
    "a = lex.lex(object=m)      # Create a lexer\n",
    "\n",
    "b = a.clone()              # Clone the lexer\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b88a7-0deb-46e5-8541-0cca847ad852",
   "metadata": {},
   "source": [
    "#### **4.17 Lexer的内部状态**\n",
    "\n",
    "lexer有一些内部属性在特定情况下有用：\n",
    "\n",
    "- `lexer.lexpos`。这是一个表示当前分析点的位置的整型值。如果你修改这个值的话，这会改变下一个`token()`的调用行为。在标记的规则方法里面，这个值表示紧跟匹配字串后面的第一个字符的位置，如果这个值在规则中修改，下一个返回的标记将从新的位置开始匹配\n",
    "- `lexer.lineno`。表示当前行号。PLY只是声明这个属性的存在，却永远不更新这个值。如果你想要跟踪行号的话，你需要自己添加代码（ [4.6 行号和位置信息](https://www.cnblogs.com/P_Chou/p/python-lex-yacc.html#4_6)）\n",
    "- `lexer.lexdata`。当前lexer的输入字串，这个字符串就是input()方法的输入字串，更改它可能是个糟糕的做法，除非你知道自己在干什么。\n",
    "- `lexer.lexmatch`。PLY内部调用Python的re.match()方法得到的当前标记的原始的Match对象，该对象被保存在这个属性中。如果你的正则式中包含分组的话，你可以通过这个对象获得这些分组的值。注意：这个属性只在有标记规则定义的方法中才有效"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4794154-c024-4447-b2df-dc00a761aeb7",
   "metadata": {},
   "source": [
    "#### **4.18 基于条件的扫描和启动条件**\n",
    "\n",
    "在高级的分析器应用程序中，使用状态化的词法扫描是很有用的。比如，你想在出现特定标记或句子结构的时候触发开始一个不同的词法分析逻辑。PLY允许lexer在不同的状态之间转换。每个状态可以包含一些自己独特的标记和规则等。这是基于GNU flex的“启动条件”来实现的。\n",
    "\n",
    "要使用lex的状态，你必须首先声明。通过在lex模块中声明\"states\"来做到：\n",
    "\n",
    "```lisp\n",
    "states = (\n",
    "   ('foo','exclusive'),\n",
    "   ('bar','inclusive'),\n",
    ")\n",
    "```\n",
    "\n",
    "这个声明中包含有两个状态：'foo'和'bar'。状态可以有两种类型：'排他型'和'包容型'。排他型的状态会使得lexer的行为发生完全的改变：只有能够匹配在这个状态下定义的规则的标记才会返回；包容型状态会将定义在这个状态下的规则添加到默认的规则集中，进而，只要能匹配这个规则集的标记都会返回。\n",
    "\n",
    "一旦声明好之后，标记规则的命名需要包含状态名：\n",
    "\n",
    "```python\n",
    "t_foo_NUMBER = r'\\d+'                      # Token 'NUMBER' in state 'foo'        \n",
    "t_bar_ID     = r'[a-zA-Z_][a-zA-Z0-9_]*'   # Token 'ID' in state 'bar'\n",
    "\n",
    "def t_foo_newline(t):\n",
    "    r'\\n'\n",
    "    t.lexer.lineno += 1\n",
    "```\n",
    "\n",
    "一个标记可以用在多个状态中，只要将多个状态名包含在声明中：\n",
    "\n",
    "```python\n",
    "t_foo_bar_NUMBER = r'\\d+'         # Defines token 'NUMBER' in both state 'foo' and 'bar'\n",
    "```\n",
    "\n",
    "同样的，在任何状态下都生效的声明可以在命名中使用`ANY`：\n",
    "\n",
    "```python\n",
    "t_ANY_NUMBER = r'\\d+'         # Defines a token 'NUMBER' in all states\n",
    "```\n",
    "\n",
    "不包含状态名的情况下，标记被关联到一个特殊的状态`INITIAL`，比如，下面两个声明是等价的：\n",
    "\n",
    "```python\n",
    "t_NUMBER = r'\\d+'\n",
    "t_INITIAL_NUMBER = r'\\d+'\n",
    "```\n",
    "\n",
    "特殊的`t_ignore()`和`t_error()`也可以用状态关联：\n",
    "\n",
    "```python\n",
    "t_foo_ignore = \" \\t\\n\"       # Ignored characters for state 'foo'\n",
    "\n",
    "def t_bar_error(t):          # Special error handler for state 'bar'\n",
    "    pass \n",
    "```\n",
    "\n",
    "词法分析默认在`INITIAL`状态下工作，这个状态下包含了所有默认的标记规则定义。对于不希望使用“状态”的用户来说，这是完全透明的。在分析过程中，如果你想要改变词法分析器的这种的状态，使用`begin()`方法：\n",
    "\n",
    "```python\n",
    "def t_begin_foo(t):\n",
    "    r'start_foo'\n",
    "    t.lexer.begin('foo')             # Starts 'foo' state\n",
    "```\n",
    "\n",
    "使用`begin()`切换回初始状态：\n",
    "\n",
    "```python\n",
    "def t_foo_end(t):\n",
    "    r'end_foo'\n",
    "    t.lexer.begin('INITIAL')        # Back to the initial state\n",
    "```\n",
    "\n",
    "状态的切换可以使用栈：\n",
    "\n",
    "```python\n",
    "def t_begin_foo(t):\n",
    "    r'start_foo'\n",
    "    t.lexer.push_state('foo')             # Starts 'foo' state\n",
    "\n",
    "def t_foo_end(t):\n",
    "    r'end_foo'\n",
    "    t.lexer.pop_state()                   # Back to the previous state\n",
    "```\n",
    "\n",
    "当你在面临很多状态可以选择进入，而又仅仅想要回到之前的状态时，状态栈比较有用。\n",
    "\n",
    "举个例子会更清晰。假设你在写一个分析器想要从一堆C代码中获取任意匹配的闭合的大括号里面的部分：这意味着，当遇到起始括号'{'，你需要读取与之匹配的'}'以上的所有部分。并返回字符串。使用通常的正则表达式几乎不可能，这是因为大括号可以嵌套，而且可以有注释，字符串等干扰。因此，试图简单的匹配第一个出现的'}'是不行的。这里你可以用lex的状态来做到：\n",
    "\n",
    "```python\n",
    "# Declare the state\n",
    "states = (\n",
    "  ('ccode','exclusive'),\n",
    ")\n",
    "\n",
    "# Match the first {. Enter ccode state.\n",
    "def t_ccode(t):\n",
    "    r'\\{'\n",
    "    t.lexer.code_start = t.lexer.lexpos        # Record the starting position\n",
    "    t.lexer.level = 1                          # Initial brace level\n",
    "    t.lexer.begin('ccode')                     # Enter 'ccode' state\n",
    "\n",
    "# Rules for the ccode state\n",
    "def t_ccode_lbrace(t):     \n",
    "    r'\\{'\n",
    "    t.lexer.level +=1                \n",
    "\n",
    "def t_ccode_rbrace(t):\n",
    "    r'\\}'\n",
    "    t.lexer.level -=1\n",
    "\n",
    "    # If closing brace, return the code fragment\n",
    "    if t.lexer.level == 0:\n",
    "         t.value = t.lexer.lexdata[t.lexer.code_start:t.lexer.lexpos+1]\n",
    "         t.type = \"CCODE\"\n",
    "         t.lexer.lineno += t.value.count('\\n')\n",
    "         t.lexer.begin('INITIAL')           \n",
    "         return t\n",
    "\n",
    "# C or C++ comment (ignore)    \n",
    "def t_ccode_comment(t):\n",
    "    r'(/\\*(.|\\n)*?*/)|(//.*)'\n",
    "    pass\n",
    "\n",
    "# C string\n",
    "def t_ccode_string(t):\n",
    "   r'\\\"([^\\\\\\n]|(\\\\.))*?\\\"'\n",
    "\n",
    "# C character literal\n",
    "def t_ccode_char(t):\n",
    "   r'\\'([^\\\\\\n]|(\\\\.))*?\\''\n",
    "\n",
    "# Any sequence of non-whitespace characters (not braces, strings)\n",
    "def t_ccode_nonspace(t):\n",
    "   r'[^\\s\\{\\}\\'\\\"]+'\n",
    "\n",
    "# Ignored characters (whitespace)\n",
    "t_ccode_ignore = \" \\t\\n\"\n",
    "\n",
    "# For bad characters, we just skip over it\n",
    "def t_ccode_error(t):\n",
    "    t.lexer.skip(1)\n",
    "```\n",
    "\n",
    "这个例子中，第一个'{'使得lexer记录了起始位置，并且进入新的状态'ccode'。一系列规则用来匹配接下来的输入，这些规则只是丢弃掉标记（不返回值），如果遇到闭合右括号，t_ccode_rbrace规则收集其中所有的代码（利用先前记录的开始位置），并保存，返回的标记类型为'CCODE'，与此同时，词法分析的状态退回到初始状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44795c-256a-4d98-b144-c6e8eba6ebde",
   "metadata": {},
   "source": [
    "#### **4.19 其他问题**\n",
    "\n",
    "- lexer需要输入的是一个字符串。好在大多数机器都有足够的内存，这很少导致性能的问题。这意味着，lexer现在还不能用来处理文件流或者socket流。这主要是受到re模块的限制。\n",
    "- lexer支持用Unicode字符描述标记的匹配规则，也支持输入字串包含Unicode\n",
    "- 如果你想要向re.compile()方法提供flag，使用reflags选项：`lex.lex(reflags=re.UNICODE)`\n",
    "- 由于lexer是全部用Python写的，性能很大程度上取决于Python的re模块，即使已经尽可能的高效了。当接收极其大量的输入文件时表现并不尽人意。如果担忧性能，你可以升级到最新的Python，或者手工创建分析器，或者用C语言写lexer并做成扩展模块。\n",
    "\n",
    "如果你要创建一个手写的词法分析器并计划用在yacc.py中，只需要满足下面的要求：\n",
    "\n",
    "- 需要提供一个`token()`方法来返回下一个标记，如果没有可用的标记了，则返回None。\n",
    "- `token()`方法必须返回一个tok对象，具有`type`和`value`属性。如果行号需要跟踪的话，标记还需要定义`lineno`属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c38db-09de-4762-ac7a-ddb6d3ffcff4",
   "metadata": {},
   "source": [
    "### **5 语法分析基础**\n",
    "\n",
    "yacc.py用来对语言进行语法分析。在给出例子之前，必须提一些重要的背景知识。首先，‘语法’通常用BNF范式来表达。例如，如果想要分析简单的算术表达式，你应该首先写下无二义的文法：\n",
    "\n",
    "```\n",
    "expression : expression + term\n",
    "                       | expression - term\n",
    "                       | term\n",
    "\n",
    "term             : term * factor\n",
    "                       | term / factor\n",
    "                       | factor\n",
    "\n",
    "factor           : NUMBER\n",
    "                       | ( expression )\n",
    "```\n",
    "\n",
    "在这个文法中，像`NUMBER`,`+`,`-`,`*`,`/`的符号被称为终结符，对应原始的输入。类似`term`，`factor`等称为非终结符，它们由一系列终结符或其他规则的符号组成，用来指代语法规则。\n",
    "\n",
    "通常使用一种叫语法制导翻译的技术来指定某种语言的语义。在语法制导翻译中，符号及其属性出现在每个语法规则后面的动作中。每当一个语法被识别，动作就能够描述需要做什么。比如，对于上面给定的文法，想要实现一个简单的计算器，应该写成下面这样：\n",
    "\n",
    "```\n",
    "Grammar                                                           Action\n",
    "-------------------------------------------           -------------------------------------------- \n",
    "expression0 : expression1 + term          expression0.val = expression1.val + term.val\n",
    "                          | expression1 - term           expression0.val = expression1.val - term.val\n",
    "                          | term                                       expression0.val = term.val\n",
    "\n",
    "term0              : term1 * factor                    term0.val = term1.val * factor.val\n",
    "                          | term1 / factor                     term0.val = term1.val / factor.val\n",
    "                          | factor                                     term0.val = factor.val\n",
    "\n",
    "factor              : NUMBER                              factor.val = int(NUMBER.lexval)\n",
    "                          | ( expression )                     factor.val = expression.val\n",
    "```\n",
    "\n",
    "一种理解语法指导翻译的好方法是将符号看成对象。与符号相关的值代表了符号的“状态”（比如上面的val属性），语义行为用一组操作符号及符号值的函数或者方法来表达。\n",
    "\n",
    "Yacc用的分析技术是著名的LR分析法或者叫移进-归约分析法。LR分析法是一种自下而上的技术：首先尝试识别右部的语法规则，每当右部得到满足，相应的行为代码将被触发执行，当前右边的语法符号将被替换为左边的语法符号。（归约）\n",
    "\n",
    "LR分析法一般这样实现：将下一个符号进栈，然后结合栈顶的符号和后继符号（译者注：下一个将要输入符号），与文法中的某种规则相比较。具体的算法可以在编译器的手册中查到，下面的例子展现了如果通过上面定义的文法，来分析3 + 5 * ( 10 - 20 )这个表达式，$用来表示输入结束\n",
    "\n",
    "```\n",
    "Step Symbol Stack                                  Input Tokens            Action\n",
    "------ ------------------------------------      ----------------------     -------------------------------\n",
    "1                                                                     3 + 5 * ( 10 - 20 )$     Shift 3\n",
    "2        3                                                              + 5 * ( 10 - 20 )$     Reduce factor : NUMBER\n",
    "3        factor                                                     + 5 * ( 10 - 20 )$    Reduce term   : factor\n",
    "4        term                                                       + 5 * ( 10 - 20 )$    Reduce expr : term\n",
    "5        expr                                                        + 5 * ( 10 - 20 )$    Shift +\n",
    "6        expr +                                                        5 * ( 10 - 20 )$    Shift 5\n",
    "7        expr + 5                                                        * ( 10 - 20 )$    Reduce factor : NUMBER\n",
    "8        expr + factor                                              * ( 10 - 20 )$    Reduce term   : factor\n",
    "9        expr + term                                                * ( 10 - 20 )$    Shift *\n",
    "10      expr + term *                                                ( 10 - 20 )$    Shift (\n",
    "11      expr + term * (                                                10 - 20 )$    Shift 10\n",
    "12      expr + term * ( 10                                                - 20 )$    Reduce factor : NUMBER\n",
    "13      expr + term * ( factor                                         - 20 )$    Reduce term : factor\n",
    "14      expr + term * ( term                                           - 20 )$    Reduce expr : term\n",
    "15      expr + term * ( expr                                            - 20 )$    Shift -\n",
    "16      expr + term * ( expr -                                            20 )$    Shift 20\n",
    "17      expr + term * ( expr - 20                                            )$    Reduce factor : NUMBER\n",
    "18      expr + term * ( expr - factor                                     )$    Reduce term : factor\n",
    "19      expr + term * ( expr - term                                       )$    Reduce expr : expr - term\n",
    "20      expr + term * ( expr                                                    )$    Shift )\n",
    "21      expr + term * ( expr )                                                   $    Reduce factor : (expr)\n",
    "22      expr + term * factor                                                     $    Reduce term : term * factor\n",
    "23      expr + term                                                                     $    Reduce expr : expr + term\n",
    "24      expr                                                                                   $    Reduce expr\n",
    "25                                                                                                  $    Success!\n",
    "\n",
    "```\n",
    "\n",
    "（译者注：action里面的Shift就是进栈动作，简称移进；Reduce是归约）\n",
    "\n",
    "在分析表达式的过程中，一个相关的自动状态机和后继符号决定了下一步应该做什么。如果下一个标记看起来是一个有效语法（产生式）的一部分（通过栈上的其他项判断这一点），那么这个标记应该进栈。如果栈顶的项可以组成一个完整的右部语法规则，一般就可以进行“归约”，用产生式左边的符号代替这一组符号。当归约发生时，相应的行为动作就会执行。如果输入标记既不能移进也不能归约的话，就会发生语法错误，分析器必须进行相应的错误恢复。分析器直到栈空并且没有另外的输入标记时，才算成功。\n",
    "需要注意的是，这是基于一个有限自动机实现的，有限自动器被转化成分析表。分析表的构建比较复杂，超出了本文的讨论范围。不过，这构建过程的微妙细节能够解释为什么在上面的例子中，解析器选择在步骤`9`将标记转移到堆栈中，而不是按照规则expr : expr + term做归约。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f841f-cda8-4a3d-949b-9a5a758dbda2",
   "metadata": {},
   "source": [
    "### **6 Yacc**\n",
    "\n",
    "ply.yacc模块实现了PLY的分析功能，‘yacc’是‘Yet Another Compiler Compiler’的缩写并保留了其作为Unix工具的名字。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47896a1-ed7b-4de8-91e2-46eacc6ca6f1",
   "metadata": {},
   "source": [
    "#### **6.1 一个例子**\n",
    "\n",
    "假设你希望实现上面的简单算术表达式的语法分析，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98af91ab-f88c-4d3f-affd-ee092779d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yacc example\n",
    "# yacc_example.py\n",
    "import ply.yacc as yacc\n",
    "\n",
    "# Get the token map from the lexer.  This is required.\n",
    "from calclex import tokens\n",
    "\n",
    "def p_expression_plus(p):\n",
    "    'expression : expression PLUS term'\n",
    "    p[0] = p[1] + p[3]\n",
    "\n",
    "def p_expression_minus(p):\n",
    "    'expression : expression MINUS term'\n",
    "    p[0] = p[1] - p[3]\n",
    "\n",
    "def p_expression_term(p):\n",
    "    'expression : term'\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_term_times(p):\n",
    "    'term : term TIMES factor'\n",
    "    p[0] = p[1] * p[3]\n",
    "\n",
    "def p_term_div(p):\n",
    "    'term : term DIVIDE factor'\n",
    "    p[0] = p[1] / p[3]\n",
    "\n",
    "def p_term_factor(p):\n",
    "    'term : factor'\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_factor_num(p):\n",
    "    'factor : NUMBER'\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_factor_expr(p):\n",
    "    'factor : LPAREN expression RPAREN'\n",
    "    p[0] = p[2]\n",
    "\n",
    "# Error rule for syntax errors\n",
    "def p_error(p):\n",
    "    print(\"Syntax error in input!\")\n",
    "\n",
    "# Build the parser\n",
    "# parser = yacc.yacc()  # will cause error in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839c161-b999-4bf0-8940-f393f3d439a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calcparse import parser\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        s = input('calc > ')\n",
    "    except EOFError:\n",
    "        break\n",
    "    if not s: break\n",
    "    result = parser.parse(s)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173f9eb-fc0d-4523-ba92-bf4c721e4519",
   "metadata": {},
   "source": [
    "在这个例子中，每个语法规则被定义成一个Python的方法，方法的文档字符串描述了相应的上下文无关文法，方法的语句实现了对应规则的语义行为。每个方法接受一个单独的`p`参数，`p`是一个包含有当前匹配语法的符号的序列，`p[i]`与语法符号的对应关系如下：\n",
    "\n",
    "```python\n",
    "def p_expression_plus(p):\n",
    "    'expression : expression PLUS term'\n",
    "    #   ^                        ^                  ^         ^\n",
    "    #  p[0]                  p[1]            p[2]    p[3]\n",
    "\n",
    "    p[0] = p[1] + p[3]\n",
    "```\n",
    "\n",
    "其中，`p[i]`的值相当于词法分析模块中对`p.value`属性赋的值，对于非终结符的值，将在归约时由`p[0]`的赋值决定，这里的值可以是任何类型，当然，大多数情况下只是Python的简单类型、元组或者类的实例。在这个例子中，我们依赖这样一个事实：`NUMBER`标记的值保存的是整型值，所有规则的行为都是得到这些整型值的算术运算结果，并传递结果。\n",
    "\n",
    "注意：在这里负数的下标有特殊意义--这里的p[-1]不等同于p[3]。详见下面的 _嵌入式动作_ 部分\n",
    "\n",
    "在yacc中定义的第一个语法规则被默认为起始规则（这个例子中的第一个出现的expression规则）。一旦起始规则被分析器归约，而且再无其他输入，分析器终止，最后的值将返回（这个值将是起始规则的p[0]）。注意：也可以通过在`yacc()`中使用start关键字参数来指定起始规则\n",
    "\n",
    "`p_error`(p)规则用于捕获语法错误。详见 _处理语法错误_ 部分\n",
    "\n",
    "为了构建分析器，需要调用`yacc.yacc()`方法。这个方法查看整个当前模块，然后试图根据你提供的文法构建LR分析表。第一次执行`yacc.yacc()`，你会得到如下输出：\n",
    "\n",
    "```ruby\n",
    "$ python calcparse.py\n",
    "Generating LALR tables\n",
    "calc >\n",
    "```\n",
    "\n",
    "由于分析表的得出相对开销较大（尤其包含大量的语法的情况下），分析表被写入当前目录的一个叫`parsetab.py`的文件中。除此之外，会生成一个调试文件`parser.out`。在接下来的执行中，yacc直到发现文法发生变化，才会重新生成分析表和parsetab.py文件，否则yacc会从parsetab.py中加载分析表。注：如果有必要的话这里输出的文件名是可以改的。\n",
    "\n",
    "如果在你的文法中有任何错误的话，yacc.py会产生调试信息，而且可能抛出异常。一些可以被检测到的错误如下：\n",
    "\n",
    "- 方法重复定义（在语法文件中具有相同名字的方法）\n",
    "- 二义文法产生的移进-归约和归约-归约冲突\n",
    "- 指定了错误的文法\n",
    "- 不可终止的递归（规则永远无法终结）\n",
    "- 未使用的规则或标记\n",
    "- 未定义的规则或标记\n",
    "\n",
    "下面几个部分将更详细的讨论语法规则\n",
    "\n",
    "这个例子的最后部分展示了如何执行由`yacc()`方法创建的分析器。你只需要简单的调用`parse()`，并将输入字符串作为参数就能运行分析器。它将运行所有的语法规则，并返回整个分析的结果，这个结果就是在起始规则中赋给`p[0]`的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ff0c0-d7f3-473b-818d-d640194eb9f0",
   "metadata": {},
   "source": [
    "#### **6.2 将语法规则合并**\n",
    "\n",
    "如果语法规则类似的话，可以合并到一个方法中。例如，考虑前面例子中的两个规则：\n",
    "\n",
    "```python\n",
    "def p_expression_plus(p):\n",
    "    'expression : expression PLUS term'\n",
    "    p[0] = p[1] + p[3]\n",
    "\n",
    "def p_expression_minus(t):\n",
    "    'expression : expression MINUS term'\n",
    "    p[0] = p[1] - p[3]\n",
    "```\n",
    "\n",
    "比起写两个方法，你可以像下面这样写在一个方法里面：\n",
    "\n",
    "```python\n",
    "def p_expression(p):\n",
    "    '''expression : expression PLUS term\n",
    "                  | expression MINUS term'''\n",
    "    if p[2] == '+':\n",
    "        p[0] = p[1] + p[3]\n",
    "    elif p[2] == '-':\n",
    "        p[0] = p[1] - p[3]\n",
    "```\n",
    "\n",
    "总之，方法的文档字符串可以包含多个语法规则。所以，像这样写也是合法的（尽管可能会引起困惑）：\n",
    "\n",
    "```python\n",
    "def p_binary_operators(p):\n",
    "    '''expression : expression PLUS term\n",
    "                  | expression MINUS term\n",
    "       term       : term TIMES factor\n",
    "                  | term DIVIDE factor'''\n",
    "    if p[2] == '+':\n",
    "        p[0] = p[1] + p[3]\n",
    "    elif p[2] == '-':\n",
    "        p[0] = p[1] - p[3]\n",
    "    elif p[2] == '*':\n",
    "        p[0] = p[1] * p[3]\n",
    "    elif p[2] == '/':\n",
    "        p[0] = p[1] / p[3]\n",
    "```\n",
    "\n",
    "如果所有的规则都有相似的结构，那么将语法规则合并才是个不错的注意（比如，产生式的项数相同）。不然，语义动作可能会变得复杂。不过，简单情况下，可以使用len()方法区分，比如：\n",
    "\n",
    "```python\n",
    "def p_expressions(p):\n",
    "    '''expression : expression MINUS expression\n",
    "                  | MINUS expression'''\n",
    "    if (len(p) == 4):\n",
    "        p[0] = p[1] - p[3]\n",
    "    elif (len(p) == 3):\n",
    "        p[0] = -p[2]\n",
    "```\n",
    "\n",
    "如果考虑解析的性能，你应该避免像这些例子一样在一个语法规则里面用很多条件来处理。因为，每次检查当前究竟匹配的是哪个语法规则的时候，实际上重复做了分析器已经做过的事（分析器已经准确的知道哪个规则被匹配了）。为每个规则定义单独的方法，可以消除这点开销。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112435a-4f06-49de-bc65-83253c2d21c7",
   "metadata": {},
   "source": [
    "#### **6.3 字面字符**\n",
    "\n",
    "如果愿意，可以在语法规则里面使用单个的字面字符，例如：\n",
    "\n",
    "```python\n",
    "def p_binary_operators(p):\n",
    "    '''expression : expression '+' term\n",
    "                  | expression '-' term\n",
    "       term       : term '*' factor\n",
    "                  | term '/' factor'''\n",
    "    if p[2] == '+':\n",
    "        p[0] = p[1] + p[3]\n",
    "    elif p[2] == '-':\n",
    "        p[0] = p[1] - p[3]\n",
    "    elif p[2] == '*':\n",
    "        p[0] = p[1] * p[3]\n",
    "    elif p[2] == '/':\n",
    "        p[0] = p[1] / p[3]\n",
    "```\n",
    "\n",
    "字符必须像'+'那样使用单引号。除此之外，需要将用到的字符定义单独定义在lex文件的`literals`列表里：\n",
    "\n",
    "```ini\n",
    "# Literals.  Should be placed in module given to lex()\n",
    "literals = ['+','-','*','/' ]\n",
    "```\n",
    "\n",
    "**字面的字符只能是单个字符**。因此，像'<='或者'=='都是不合法的，只能使用一般的词法规则（例如t_EQ = r'==')。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23003f43-6f75-4638-ab78-1c9c56ec51e6",
   "metadata": {},
   "source": [
    "#### **6.4 空产生式**\n",
    "\n",
    "yacc.py可以处理空产生式，像下面这样做：\n",
    "\n",
    "```python\n",
    "def p_empty(p):\n",
    "    'empty :'\n",
    "    pass\n",
    "```\n",
    "\n",
    "现在可以使用空匹配，只要将'`empty`'当成一个符号使用：\n",
    "\n",
    "```python\n",
    "def p_optitem(p):\n",
    "    'optitem : item'\n",
    "    '        | empty'\n",
    "    ...\n",
    "```\n",
    "\n",
    "注意：你可以将产生式保持'空'，来表示空匹配。然而，我发现用一个'`empty`'规则并用其来替代'空'，更容易表达意图，并有较好的可读性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411452fe-5e30-4853-97cb-1f04fc254f14",
   "metadata": {},
   "source": [
    "#### **6.5 改变起始符号**\n",
    "\n",
    "默认情况下，在yacc中的第一条规则是起始语法规则（顶层规则）。可以用`start`标识来改变这种行为：\n",
    "\n",
    "```python\n",
    "start = 'foo'\n",
    "\n",
    "def p_bar(p):\n",
    "    'bar : A B'\n",
    "\n",
    "# This is the starting rule due to the start specifier above\n",
    "def p_foo(p):\n",
    "    'foo : bar X'\n",
    "...\n",
    "```\n",
    "\n",
    "用`start`标识有助于在调试的时候将大型的语法规则分成小部分来分析。也可把start符号作为yacc的参数：\n",
    "\n",
    "```sql\n",
    "yacc.yacc(start='foo')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f40ab0-bc3b-454a-88f8-1ee6ac7bf297",
   "metadata": {},
   "source": [
    "#### **6.6 处理二义文法**\n",
    "\n",
    "上面例子中，对表达式的文法描述用一种特别的形式规避了二义文法。然而，在很多情况下，这样的特殊文法很难写，或者很别扭。一个更为自然和舒服的语法表达应该是这样的：\n",
    "\n",
    "```r\n",
    "expression : expression PLUS expression\n",
    "                        | expression MINUS expression\n",
    "                        | expression TIMES expression\n",
    "                        | expression DIVIDE expression\n",
    "                        | LPAREN expression RPAREN\n",
    "                        | NUMBER\n",
    "```\n",
    "\n",
    "不幸的是，这样的文法是存在二义性的。举个例子，如果你要解析字符串\"3 * 4 + 5\"，操作符如何分组并没有指明，究竟是表示\"(3 * 4) + 5\"还是\"3 * (4 + 5)\"呢？\n",
    "\n",
    "如果在yacc.py中存在二义文法，会输出\"移进归约冲突\"或者\"归约归约冲突\"。在分析器无法确定是将下一个符号移进栈还是将当前栈中的符号归约时会产生移进归约冲突。例如，对于\"3 * 4 + 5\"，分析器内部栈是这样工作的：\n",
    "\n",
    "```ruby\n",
    "Step Symbol Stack           Input Tokens            Action\n",
    "---- ---------------------  ---------------------   -------------------------------\n",
    "1    $                                                   3 * 4 + 5$    Shift 3\n",
    "2    $ 3                                                   * 4 + 5$    Reduce : expression : NUMBER\n",
    "3    $ expr                                            * 4 + 5$    Shift *\n",
    "4    $ expr *                                            4 + 5$    Shift 4\n",
    "5    $ expr * 4                                            + 5$    Reduce: expression : NUMBER\n",
    "6    $ expr * expr                                     + 5$    SHIFT/REDUCE CONFLICT ????\n",
    "```\n",
    "\n",
    "在这个例子中，当分析器来到第6步的时候，有两种选择：一是按照expr : expr * expr归约，一是将标记'+'继续移进栈。两种选择对于上面的上下文无关文法而言都是合法的。\n",
    "\n",
    "默认情况下，所有的移进归约冲突会倾向于使用移进来处理。因此，对于上面的例子，分析器总是会将'+'进栈，而不是做归约。虽然在很多情况下，这个策略是合适的（像\"if-then\"和\"if-then-else\"），但这对于算术表达式是不够的。事实上，对于上面的例子，将'+'进栈是完全错误的，应当先将expr * expr归约，因为乘法的优先级要高于加法。\n",
    "\n",
    "为了解决二义文法，尤其是对表达式文法，yacc.py允许为标记单独指定优先级和结合性。需要像下面这样增加一个precedence变量：\n",
    "\n",
    "```lisp\n",
    "precedence = (\n",
    "    ('left', 'PLUS', 'MINUS'),\n",
    "    ('left', 'TIMES', 'DIVIDE'),\n",
    ")\n",
    "```\n",
    "\n",
    "这样的定义说明`PLUS/MINUS`标记具有相同的优先级和左结合性，`TIMES/DIVIDE`具有相同的优先级和左结合性。在precedence声明中，标记的优先级从低到高。因此，这个声明表明`TIMES/DIVIDE`（他们较晚加入`precedence`）的优先级高于`PLUS/MINUS`。\n",
    "\n",
    "由于为标记添加了数字表示的优先级和结合性的属性，所以，对于上面的例子，将会得到：\n",
    "\n",
    "```bash\n",
    "PLUS         : level = 1,  assoc = 'left'\n",
    "MINUS      : level = 1,  assoc = 'left'\n",
    "TIMES       : level = 2,  assoc = 'left'\n",
    "DIVIDE      : level = 2,  assoc = 'left'\n",
    "```\n",
    "\n",
    "随后这些值被附加到语法规则的优先级和结合性属性上，**这些值由最右边的终结符的优先级和结合性决定**：\n",
    "\n",
    "```r\n",
    "expression : expression PLUS expression                   # level = 1, left\n",
    "                        | expression MINUS expression               # level = 1, left\n",
    "                        | expression TIMES expression                # level = 2, left\n",
    "                        | expression DIVIDE expression               # level = 2, left\n",
    "                        | LPAREN expression RPAREN                  # level = None (not specified)\n",
    "                        | NUMBER                                                         # level = None (not specified)\n",
    "```\n",
    "\n",
    "当出现移进归约冲突时，分析器生成器根据下面的规则解决二义文法：\n",
    "\n",
    "1. 如果当前的标记的优先级高于栈顶规则的优先级，移进当前标记\n",
    "2. 如果栈顶规则的优先级更高，进行归约\n",
    "3. 如果当前的标记与栈顶规则的优先级相同，如果标记是左结合的，则归约，否则，如果是右结合的则移进\n",
    "4. 如果没有优先级可以参考，默认对于移进归约冲突执行移进\n",
    "\n",
    "比如，当解析到\"expression PLUS expression\"这个语法时，下一个标记是`TIMES`，此时将执行移进，因为`TIMES`具有比`PLUS`更高的优先级；当解析到\"expression TIMES expression\"，下一个标记是`PLUS`，此时将执行归约，因为`PLUS`的优先级低于`TIMES`。\n",
    "\n",
    "如果在使用前三种技术解决已经归约冲突后，yacc.py将不会报告语法中的冲突或者错误（不过，会在parser.out这个调试文件中输出一些信息）\n",
    "\n",
    "使用`precedence`指定优先级的技术会带来一个问题，有时运算符的优先级需要基于上下文。例如，考虑\"3 + 4 * -5\"中的一元的'-'。数学上讲，一元运算符应当拥有较高的优先级。然而，在我们的`precedence`定义中，`MINUS`的优先级却低于`TIMES`。为了解决这个问题，`precedene`规则中可以包含\"虚拟标记\"：\n",
    "\n",
    "```lisp\n",
    "precedence = (\n",
    "    ('left', 'PLUS', 'MINUS'),\n",
    "    ('left', 'TIMES', 'DIVIDE'),\n",
    "    ('right', 'UMINUS'),            # Unary minus operator\n",
    ")\n",
    "```\n",
    "\n",
    "在语法文件中，我们可以这么表示一元算符：\n",
    "\n",
    "```python\n",
    "def p_expr_uminus(p):\n",
    "    'expression : MINUS expression %prec UMINUS'\n",
    "    p[0] = -p[2]\n",
    "```\n",
    "\n",
    "在这个例子中，`%prec UMINUS`覆盖了默认的优先级（`MINUS`的优先级），将`UMINUS`指代的优先级应用在该语法规则上。\n",
    "\n",
    "起初，`UMINUS`标记的例子会让人感到困惑。`UMINUS`既不是输入的标记也不是语法规则，你应当将其看成`precedence`表中的特殊的占位符。当你使用`%prec`宏时，你是在告诉yacc，你希望表达式使用这个占位符所表示的优先级，而不是正常的优先级。\n",
    "\n",
    "还可以在`precedence`表中指定\"非关联\"。这表明你不希望链式运算符。比如，假如你希望支持比较运算符'<'和'>'，但是你不希望支持 a < b < c，只要简单指定规则如下：\n",
    "\n",
    "```lisp\n",
    "precedence = (\n",
    "    ('nonassoc', 'LESSTHAN', 'GREATERTHAN'),  # Nonassociative operators\n",
    "    ('left', 'PLUS', 'MINUS'),\n",
    "    ('left', 'TIMES', 'DIVIDE'),\n",
    "    ('right', 'UMINUS'),            # Unary minus operator\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "此时，当输入形如 a < b < c时，将产生语法错误，却不影响形如 a < b 的表达式。\n",
    "\n",
    " \n",
    "\n",
    "对于给定的符号集，存在多种语法规则可以匹配时会产生归约/归约冲突。这样的冲突往往很严重，而且总是通过匹配最早出现的语法规则来解决。归约/归约冲突几乎总是相同的符号集合具有不同的规则可以匹配，而在这一点上无法抉择，比如：\n",
    "\n",
    "```r\n",
    "assignment :  ID EQUALS NUMBER\n",
    "                         |  ID EQUALS expression\n",
    "           \n",
    "expression : expression PLUS expression\n",
    "                       | expression MINUS expression\n",
    "                       | expression TIMES expression\n",
    "                       | expression DIVIDE expression\n",
    "                       | LPAREN expression RPAREN\n",
    "                       | NUMBER\n",
    "```\n",
    "\n",
    "这个例子中，对于下面这两条规则将产生归约/归约冲突：\n",
    "\n",
    "```yaml\n",
    "assignment  : ID EQUALS NUMBER\n",
    "expression    : NUMBER\n",
    "```\n",
    "\n",
    "比如，对于\"a = 5\"，分析器不知道应当按照assignment : ID EQUALS NUMBER归约，还是先将5归约成expression，再归约成assignment : ID EQUALS expression。\n",
    "\n",
    "应当指出的是，只是简单的查看语法规则是很难减少归约/归约冲突。如果出现归约/归约冲突，yacc()会帮助打印出警告信息：\n",
    "\n",
    "```vbnet\n",
    "WARNING: 1 reduce/reduce conflict\n",
    "WARNING: reduce/reduce conflict in state 15 resolved using rule (assignment -> ID EQUALS NUMBER)\n",
    "WARNING: rejected rule (expression -> NUMBER)\n",
    "```\n",
    "\n",
    "上面的信息标识出了冲突的两条规则，但是，并无法指出究竟在什么情况下会出现这样的状态。想要发现问题，你可能需要结合语法规则和parser.out调试文件的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4aa8a3-e2fb-45a0-bcc4-0d8ed603dcc5",
   "metadata": {},
   "source": [
    "#### **6.8 处理语法错误**\n",
    "\n",
    "如果你创建的分析器用于产品，处理语法错误是很重要的。一般而言，你不希望分析器在遇到错误的时候就抛出异常并终止，相反，你需要它报告错误，尽可能的恢复并继续分析，一次性的将输入中所有的错误报告给用户。这是一些已知语言编译器的标准行为，例如C,C++,Java。在PLY中，在语法分析过程中出现错误，错误会被立即检测到（分析器不会继续读取源文件中错误点后面的标记）。然而，这时，分析器会进入恢复模式，这个模式能够用来尝试继续向下分析。LR分析器的错误恢复是个理论与技巧兼备的问题，yacc.py提供的错误机制与Unix下的yacc类似，所以你可以从诸如O'Reilly出版的《Lex and yacc》的书中找到更多的细节。\n",
    "\n",
    "当错误发生时，yacc.py按照如下步骤进行：\n",
    "\n",
    "1. 第一次错误产生时，用户定义的`p_error()`方法会被调用，出错的标记会作为参数传入；如果错误是因为到达文件结尾造成的，传入的参数将为`None`。随后，分析器进入到“错误恢复”模式，该模式下不会在产生`p_error()`调用，直到它成功的移进3个标记，然后回归到正常模式。\n",
    "2. 如果在`p_error()`中没有指定恢复动作的话，这个导致错误的标记会被替换成一个特殊的`error`标记。\n",
    "3. 如果导致错误的标记已经是`error`的话，原先的栈顶的标记将被移除。\n",
    "4. 如果整个分析栈被放弃，分析器会进入重置状态，并从他的初始状态开始分析。\n",
    "5. 如果此时的语法规则接受`error`标记，`error`标记会移进栈。\n",
    "6. 如果当前栈顶是`error`标记，之后的标记将被忽略，直到有标记能够导致`error`的归约。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63a38c-9494-47ef-a0b5-56ef4758823e",
   "metadata": {},
   "source": [
    "##### **6.8.1 根据error规则恢复和再同步**\n",
    "\n",
    "最佳的处理语法错误的做法是在语法规则中包含`error`标记。例如，假设你的语言有一个关于print的语句的语法规则：\n",
    "\n",
    "```python\n",
    "def p_statement_print(p):\n",
    "     'statement : PRINT expr SEMI'\n",
    "     ...\n",
    "```\n",
    "\n",
    "为了处理可能的错误表达式，你可以添加一条额外的语法规则：\n",
    "\n",
    "```python\n",
    "def p_statement_print_error(p):\n",
    "     'statement : PRINT error SEMI'\n",
    "     print \"Syntax error in print statement. Bad expression\"\n",
    "```\n",
    "\n",
    "这样（expr错误时），`error`标记会匹配任意多个分号之前的标记（分号是`SEMI`指代的字符）。一旦找到分号，规则将被匹配，这样`error`标记就被归约了。\n",
    "\n",
    "这种类型的恢复有时称为\"分析器再同步\"。`error`标记扮演了表示所有错误标记的通配符的角色，而紧随其后的标记扮演了同步标记的角色。\n",
    "\n",
    "重要的一个说明是，通常`error`不会作为语法规则的最后一个标记，像这样：\n",
    "\n",
    "```python\n",
    "def p_statement_print_error(p):\n",
    "    'statement : PRINT error'\n",
    "    print \"Syntax error in print statement. Bad expression\"\n",
    "```\n",
    "\n",
    "这是因为，第一个导致错误的标记会使得该规则立刻归约，进而使得在后面还有错误标记的情况下，恢复变得困难。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9d87f-4f6e-423e-89dd-fd702d43c2e2",
   "metadata": {},
   "source": [
    "##### **6.8.2 悲观恢复模式**\n",
    "\n",
    "另一个错误恢复方法是采用“悲观模式”：该模式下，开始放弃剩余的标记，直到能够达到一个合适的恢复机会。\n",
    "\n",
    "悲观恢复模式都是在`p_error()`方法中做到的。例如，这个方法在开始丢弃标记后，直到找到闭合的'}'，才重置分析器到初始化状态：\n",
    "\n",
    "```python\n",
    "def p_error(p):\n",
    "    print \"Whoa. You are seriously hosed.\"\n",
    "    # Read ahead looking for a closing '}'\n",
    "    while 1:\n",
    "        tok = yacc.token()             # Get the next token\n",
    "        if not tok or tok.type == 'RBRACE': break\n",
    "    yacc.restart()\n",
    "```\n",
    "\n",
    "\n",
    "下面这个方法简单的抛弃错误的标记，并告知分析器错误被接受了：\n",
    "\n",
    "```python\n",
    "def p_error(p):\n",
    "    print \"Syntax error at token\", p.type\n",
    "    # Just discard the token and tell the parser it's okay.\n",
    "    yacc.errok()\n",
    "```\n",
    "\n",
    "在p_error()方法中，有三个可用的方法来控制分析器的行为：\n",
    "\n",
    "- `yacc.errok()` 这个方法将分析器从恢复模式切换回正常模式。这会使得不会产生`error`标记，并重置内部的`error`计数器，而且下一个语法错误会再次产生`p_error()`调用\n",
    "- `yacc.token()` 这个方法用于得到下一个标记\n",
    "- `yacc.restart()` 这个方法抛弃当前整个分析栈，并重置分析器为起始状态\n",
    "\n",
    "注意：这三个方法只能在`p_error()`中使用，不能用在其他任何地方。\n",
    "\n",
    "`p_error()`方法也可以返回标记，这样能够控制将哪个标记作为下一个标记返回给分析器。这对于需要同步一些特殊标记的时候有用，比如：\n",
    "\n",
    "```python\n",
    "def p_error(p):\n",
    "    # Read ahead looking for a terminating \";\"\n",
    "    while 1:\n",
    "        tok = yacc.token()             # Get the next token\n",
    "        if not tok or tok.type == 'SEMI': break\n",
    "    yacc.errok()\n",
    "\n",
    "    # Return SEMI to the parser as the next lookahead token\n",
    "    return tok\n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb85e9-c14f-4ba2-966a-ce96f1b4ced4",
   "metadata": {},
   "source": [
    "##### **6.8.3 从产生式中抛出错误**\n",
    "\n",
    "如果有需要的话，产生式规则可以主动的使分析器进入恢复模式。这是通过抛出`SyntaxError`异常做到的：\n",
    "\n",
    "```python\n",
    "def p_production(p):\n",
    "    'production : some production ...'\n",
    "    raise SyntaxError\n",
    "```\n",
    "\n",
    "`raise SyntaxError`错误的效果就如同当前的标记是错误标记一样。因此，当你这么做的话，最后一个标记将被弹出栈，当前的下一个标记将是`error`标记，分析器进入恢复模式，试图归约满足`error`标记的规则。此后的步骤与检测到语法错误的情况是完全一样的，`p_error()`也会被调用。\n",
    "\n",
    "手动设置错误有个重要的方面，就是`p_error()`方法在这种情况下不会调用。如果你希望记录错误，确保在抛出`SyntaxError`错误的产生式中实现。\n",
    "\n",
    "注：这个功能是为了模仿yacc中的`YYERROR`宏的行为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5efdd2-5303-4786-9d31-6b35bf34f8a8",
   "metadata": {},
   "source": [
    "##### **6.8.4 错误恢复总结**\n",
    "\n",
    "对于通常的语言，使用`error`规则和再同步标记可能是最合理的手段。这是因为你可以将语法设计成在一个相对容易恢复和继续分析的点捕获错误。悲观恢复模式只在一些十分特殊的应用中有用，这些应用往往需要丢弃掉大量输入，再寻找合理的同步点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c95e8e-dabc-41b5-ba7b-5e8698b1f597",
   "metadata": {},
   "source": [
    "#### **6.9 行号和位置的跟踪**\n",
    "\n",
    "位置跟踪通常是个设计编译器时的技巧性玩意儿。默认情况下，PLY跟踪所有标记的行号和位置，这些信息可以这样得到：\n",
    "\n",
    "- `p.lineno(num)`返回第num个符号的行号\n",
    "- `p.lexpos(num)`返回第num个符号的词法位置偏移\n",
    "\n",
    "例如：\n",
    "\n",
    "```sql\n",
    "def p_expression(p):\n",
    "    'expression : expression PLUS expression'\n",
    "    p.lineno(1)        # Line number of the left expression\n",
    "    p.lineno(2)        # line number of the PLUS operator\n",
    "    p.lineno(3)        # line number of the right expression\n",
    "    ...\n",
    "    start,end = p.linespan(3)    # Start,end lines of the right expression\n",
    "    starti,endi = p.lexspan(3)   # Start,end positions of right expression\n",
    "```\n",
    "\n",
    "注意：`lexspan()`方法只会返回的结束位置是最后一个符号的起始位置。\n",
    "\n",
    "虽然，PLY对所有符号的行号和位置的跟踪很管用，但经常是不必要的。例如，你仅仅是在错误信息中使用行号，你通常可以仅仅使用关键标记的信息，比如：\n",
    "\n",
    "```python\n",
    "def p_bad_func(p):\n",
    "    'funccall : fname LPAREN error RPAREN'\n",
    "    # Line number reported from LPAREN token\n",
    "    print \"Bad function call at line\", p.lineno(2)\n",
    "```\n",
    "\n",
    "类似的，为了改善性能，你可以有选择性的将行号信息在必要的时候进行传递，这是通过`p.set_lineno()`实现的，例如：\n",
    "\n",
    "```css\n",
    "def p_fname(p):\n",
    "    'fname : ID'\n",
    "    p[0] = p[1]\n",
    "    p.set_lineno(0,p.lineno(1))\n",
    "```\n",
    "\n",
    "对于已经完成分析的规则，PLY不会保留行号信息，如果你是在构建抽象语法树而且需要行号，你应该确保行号保留在树上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b67eb9-47f1-4e30-a554-84f25efbe827",
   "metadata": {},
   "source": [
    "#### **6.10 构造抽象语法树**\n",
    "\n",
    "yacc.py没有构造抽像语法树的特殊方法。不过，你可以自己很简单的构造出来。\n",
    "\n",
    "一个最为简单的构造方法是为每个语法规则创建元组或者字典，并传递它们。有很多中可行的方案，下面是一个例子：\n",
    "\n",
    "```python\n",
    "def p_expression_binop(p):\n",
    "    '''expression : expression PLUS expression\n",
    "                  | expression MINUS expression\n",
    "                  | expression TIMES expression\n",
    "                  | expression DIVIDE expression'''\n",
    "\n",
    "    p[0] = ('binary-expression',p[2],p[1],p[3])\n",
    "\n",
    "def p_expression_group(p):\n",
    "    'expression : LPAREN expression RPAREN'\n",
    "    p[0] = ('group-expression',p[2])\n",
    "\n",
    "def p_expression_number(p):\n",
    "    'expression : NUMBER'\n",
    "    p[0] = ('number-expression',p[1])\n",
    "```\n",
    "\n",
    "另一种方法可以是为不同的抽象树节点创建一系列的数据结构，并赋值给`p[0]`：\n",
    "\n",
    "```python\n",
    "class Expr: pass\n",
    "\n",
    "class BinOp(Expr):\n",
    "    def __init__(self,left,op,right):\n",
    "        self.type = \"binop\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.op = op\n",
    "\n",
    "class Number(Expr):\n",
    "    def __init__(self,value):\n",
    "        self.type = \"number\"\n",
    "        self.value = value\n",
    "\n",
    "def p_expression_binop(p):\n",
    "    '''expression : expression PLUS expression\n",
    "                  | expression MINUS expression\n",
    "                  | expression TIMES expression\n",
    "                  | expression DIVIDE expression'''\n",
    "\n",
    "    p[0] = BinOp(p[1],p[2],p[3])\n",
    "\n",
    "def p_expression_group(p):\n",
    "    'expression : LPAREN expression RPAREN'\n",
    "    p[0] = p[2]\n",
    "\n",
    "def p_expression_number(p):\n",
    "    'expression : NUMBER'\n",
    "    p[0] = Number(p[1])\n",
    "```\n",
    "\n",
    "这种方式的好处是在处理复杂语义时比较简单：类型检查、代码生成、以及其他针对树节点的功能。\n",
    "\n",
    "为了简化树的遍历，可以创建一个通用的树节点结构，例如：\n",
    "\n",
    "```python\n",
    "class Node:\n",
    "    def __init__(self,type,children=None,leaf=None):\n",
    "         self.type = type\n",
    "         if children:\n",
    "              self.children = children\n",
    "         else:\n",
    "              self.children = [ ]\n",
    "         self.leaf = leaf\n",
    "         \n",
    "def p_expression_binop(p):\n",
    "    '''expression : expression PLUS expression\n",
    "                  | expression MINUS expression\n",
    "                  | expression TIMES expression\n",
    "                  | expression DIVIDE expression'''\n",
    "\n",
    "    p[0] = Node(\"binop\", [p[1],p[3]], p[2])\n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f9f7b-a91a-49ee-924e-499232b41bd3",
   "metadata": {},
   "source": [
    "#### **6.11 嵌入式动作**\n",
    "\n",
    "yacc使用的分析技术只允许在规则规约后执行动作。假设有如下规则：\n",
    "\n",
    "```python\n",
    "def p_foo(p):\n",
    "    \"foo : A B C D\"\n",
    "    print \"Parsed a foo\", p[1],p[2],p[3],p[4]\n",
    "```\n",
    "\n",
    "方法只会在符号A,B,C和D都完成后才能执行。可是有的时候，在中间阶段执行一小段代码是有用的。假如，你想在A完成后立即执行一些动作，像下面这样用空规则：\n",
    "\n",
    "```python\n",
    "def p_foo(p):\n",
    "    \"foo : A seen_A B C D\"\n",
    "    print \"Parsed a foo\", p[1],p[3],p[4],p[5]\n",
    "    print \"seen_A returned\", p[2]\n",
    "\n",
    "def p_seen_A(p):\n",
    "    \"seen_A :\"\n",
    "    print \"Saw an A = \", p[-1]   # Access grammar symbol to left\n",
    "    p[0] = some_value            # Assign value to seen_A\n",
    "```\n",
    "\n",
    "在这个例子中，空规则`seen_A`将在A移进分析栈后立即执行。`p[-1]`指代的是在分析栈上紧跟在seen_A左侧的符号。在这个例子中，是A符号。像其他普通的规则一样，在嵌入式行为中也可以通过为`p[0]`赋值来返回某些值。\n",
    "\n",
    "使用嵌入式动作可能会导致移进归约冲突，比如，下面的语法是没有冲突的：\n",
    "\n",
    "```python\n",
    "def p_foo(p):\n",
    "    \"\"\"foo : abcd\n",
    "           | abcx\"\"\"\n",
    "\n",
    "def p_abcd(p):\n",
    "    \"abcd : A B C D\"\n",
    "\n",
    "def p_abcx(p):\n",
    "    \"abcx : A B C X\"\n",
    "```\n",
    "\n",
    "可是，如果像这样插入一个嵌入式动作：\n",
    "\n",
    "```python\n",
    "def p_foo(p):\n",
    "    \"\"\"foo : abcd\n",
    "           | abcx\"\"\"\n",
    "\n",
    "def p_abcd(p):\n",
    "    \"abcd : A B C D\"\n",
    "\n",
    "def p_abcx(p):\n",
    "    \"abcx : A B seen_AB C X\"\n",
    "\n",
    "def p_seen_AB(p):\n",
    "    \"seen_AB :\"\n",
    "```\n",
    "\n",
    "会产生移进归约冲，只是由于对于两个规则abcd和abcx中的C，分析器既可以根据abcd规则移进，也可以根据abcx规则先将空的`seen_AB`归约。\n",
    "\n",
    "嵌入动作的一般用于分析以外的控制，比如为本地变量定义作用于。对于C语言：\n",
    "\n",
    "```python\n",
    "def p_statements_block(p):\n",
    "    \"statements: LBRACE new_scope statements RBRACE\"\"\"\n",
    "    # Action code\n",
    "    ...\n",
    "    pop_scope()        # Return to previous scope\n",
    "\n",
    "def p_new_scope(p):\n",
    "    \"new_scope :\"\n",
    "    # Create a new scope for local variables\n",
    "    s = new_scope()\n",
    "    push_scope(s)\n",
    "    ...\n",
    "```\n",
    "\n",
    "在这个例子中，new_scope作为嵌入式行为，在左大括号`{`之后立即执行。可以是调正内部符号表或者其他方面。`statements_block`一完成，代码可能会撤销在嵌入动作时的操作（比如，pop_scope())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c349555-ac62-4339-95d0-ff086aa42490",
   "metadata": {},
   "source": [
    "### **7 多个语法和词法分析器**\n",
    "\n",
    "在高级的分析器程序中，你可能同时需要多个语法和词法分析器。\n",
    "\n",
    "依照规则行事不会有问题。不过，你需要小心确定所有东西都正确的绑定(hooked up)了。首先，保证将lex()和yacc()返回的对象保存起来：\n",
    "\n",
    "```ini\n",
    "lexer  = lex.lex()       # Return lexer object\n",
    "parser = yacc.yacc()     # Return parser object\n",
    "```\n",
    "\n",
    "接着，在解析时，确保给`parse()`方法一个正确的lexer引用：\n",
    "\n",
    "```scss\n",
    "parser.parse(text,lexer=lexer)\n",
    "```\n",
    "\n",
    "如果遗漏这一步，分析器会使用最新创建的lexer对象，这可能不是你希望的。\n",
    "\n",
    "词法器和语法器的方法中也可以访问这些对象。在词法器中，标记的lexer属性指代的是当前触发规则的词法器对象：\n",
    "\n",
    "```python\n",
    "def t_NUMBER(t):\n",
    "   r'\\d+'\n",
    "   ...\n",
    "   print t.lexer           # Show lexer object\n",
    "```\n",
    "\n",
    "在语法器中，lexer和parser属性指代的是对应的词法器对象和语法器对象\n",
    "\n",
    "```python\n",
    "def p_expr_plus(p):\n",
    "   'expr : expr PLUS expr'\n",
    "   ...\n",
    "   print p.parser          # Show parser object\n",
    "   print p.lexer           # Show lexer object\n",
    "```\n",
    "\n",
    "如果有必要，lexer对象和parser对象都可以附加其他属性。例如，你想要有不同的解析器状态，可以为为parser对象附加更多的属性，并在后面用到它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3e2da-2cad-43e4-a500-361fad509d59",
   "metadata": {},
   "source": [
    "### **8 使用Python的优化模式**\n",
    "\n",
    "由于PLY从文档字串中获取信息，语法解析和词法分析信息必须通过正常模式下的Python解释器得到（不带有-O或者-OO选项）。不过，如果你像这样指定`optimize`模式：\n",
    "\n",
    "```scss\n",
    "lex.lex(optimize=1)\n",
    "yacc.yacc(optimize=1)\n",
    "```\n",
    "\n",
    "PLY可以在下次执行，在Python的优化模式下执行。但你必须确保第一次执行是在Python的正常模式下进行，一旦词法分析表和语法分析表生成一次后，在Python优化模式下执行，PLY会使用生成好的分析表而不再需要文档字串。\n",
    "\n",
    "注意：在优化模式下执行PLY会禁用很多错误检查机制。你应该只在程序稳定后，不再需要调试的情况下这样做。使用优化模式的目的应该是大幅减少你的编译器的启动时间（万事俱备只欠东风时）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189a887-0d1f-4008-b6d8-c2bd8c644403",
   "metadata": {},
   "source": [
    "### **9 高级调试**\n",
    "\n",
    "调试一个编译器不是件容易的事情。PLY提供了一些高级的调试能力，这是通过Python的`logging`模块实现的，下面两节介绍这一主题：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5526f85-57e6-4086-a6a8-96938cac8f86",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **9.1 调试lex()和yacc()命令**\n",
    "\n",
    "`lex()`和`yacc()`命令都有调试模式，可以通过debug标识实现：\n",
    "\n",
    "```python\n",
    "lex.lex(debug=True)\n",
    "yacc.yacc(debug=True)\n",
    "```\n",
    "\n",
    "正常情况下，调试不仅输出标准错误，对于yacc()，还会给出parser.out文件。这些输出可以通过提供logging对象来精细的控制。下面这个例子增加了对调试信息来源的输出：\n",
    "\n",
    "```python\n",
    "# Set up a logging object\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level = logging.DEBUG,\n",
    "    filename = \"parselog.txt\",\n",
    "    filemode = \"w\",\n",
    "    format = \"%(filename)10s:%(lineno)4d:%(message)s\"\n",
    ")\n",
    "log = logging.getLogger()\n",
    "\n",
    "lex.lex(debug=True,debuglog=log)\n",
    "yacc.yacc(debug=True,debuglog=log)\n",
    "```\n",
    "\n",
    "如果你提供一个自定义的logger，大量的调试信息可以通过分级来控制。典型的是将调试信息分为`DEBUG`,`INFO`,或者`WARNING`三个级别。\n",
    "\n",
    "PLY的错误和警告信息通过日志接口提供，可以从`errorlog`参数中传入日志对象\n",
    "\n",
    "```python\n",
    "lex.lex(errorlog=log)\n",
    "yacc.yacc(errorlog=log)\n",
    "```\n",
    "\n",
    "如果想完全过滤掉警告信息，你除了可以使用带级别过滤功能的日志对象，也可以使用lex和yacc模块都内建的`Nulllogger`对象。例如：\n",
    "\n",
    "```scss\n",
    "yacc.yacc(errorlog=yacc.NullLogger())\n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb512416-ee3f-4735-b569-29704eb7c247",
   "metadata": {},
   "source": [
    "#### **9.2 运行时调试**\n",
    "\n",
    "为分析器指定debug选项，可以激活语法分析器的运行时调试功能。这个选项可以是整数（表示对调试功能是开还是关），也可以是logger对象。例如：\n",
    "\n",
    "```lua\n",
    "log = logging.getLogger()\n",
    "parser.parse(input,debug=log)\n",
    "```\n",
    "\n",
    "如果传入日志对象的话，你可以使用其级别过滤功能来控制内容的输出。`INFO`级别用来产生归约信息；`DEBUG`级别会显示分析栈的信息、移进的标记和其他详细信息。`ERROR`级别显示分析过程中的错误相关信息。\n",
    "\n",
    "对于每个复杂的问题，你应该用日志对象，以便输出重定向到文件中，进而方便在执行结束后检查。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d6c00-c968-4f07-9990-e19a958824bb",
   "metadata": {},
   "source": [
    "### **10 如何继续**\n",
    "\n",
    "PLY分发包中的`example`目录包含几个简单的示例。对于理论性的东西以及LR分析发的实现细节，应当从编译器相关的书籍中学习。\n",
    "\n",
    "| token                       | 标记                                |\n",
    "| --------------------------- | ----------------------------------- |\n",
    "| context free grammar        | 上下文无关文法                      |\n",
    "| syntax directed translation | 语法制导的翻译                      |\n",
    "| ambiguity                   | 二义                                |\n",
    "| terminals                   | 终结符                              |\n",
    "| non-terminals               | 非终结符                            |\n",
    "| documentation string        | 文档字符串（python中的_docstring_） |\n",
    "| shift-reduce                | 移进-归约                           |\n",
    "| Empty Productions           | 空产生式                            |\n",
    "| Panic mode recovery         | 悲观恢复模式                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872de00-7a28-472d-b018-a9a333b72f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
